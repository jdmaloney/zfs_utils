*** Documentation for ZFS Utils ***
Written By: JD Maloney
Storage Engineer | NCSA (National Center for Supercomputing Applications)

~~ zfs_drive_mapper ~~
This script is ideally to be run following initial zpool creation, however it can also be run
when all zpools are in a completely Healthy and Online state.  Also this script should be put in
/etc/rc.local to run everytime the machine reboots.  This will pick up new device assignments if
they change on reboot.  The zfs_fault_finder script calls the map generated by this script
to locate the slot of a failed device.  Modify the map_path variable to the desired output for 
this script, make sure it matches in all zfs scripts, default is /root/disk_map.  


~~ zfs_enclosure_mapper ~~
Sample script for a 45-bay Supermicro JBOD Chassis.  Will need modified for any other chassis style
however it provides a framework for using with other enclosurers.  This script is called by
zfs_fault_finder when a failure has been detected to generate the map that is put into the email
to the storage admin, for visual reference for the failed disk(s).


~~ zfs_fault_finder ~~
Script that monitors the zpools, watching for failures and reporting what it finds to the storage 
admin whose information is in the admin_email variable.  Recommendation is a softlink to the script
be placed in /etc/cron.hourly however it can be run at any desired interval.  Note that the frequency
this script runs will be the frequency with which failure and rebuild progress reports will be emailed
to the storage admin.  The fail_path variable is the location where the file puts a list of the currently
failed drives, this must match the path in zfs_fault_fixer.  Note: Some slight enclosure path naming may need
customized on different system, change very minor.  


~~ zfs_fault_fixer ~~
This script is run by hand by the storage admin after replacing the failed drive(s) in the bay(s).  It 
shuts off the failure light on the tray(s) and begins the rebuild process on the new drive(s).  Autmating 
the script is not recommened in many cases as a new drive must be put in before running this script to avoid
running into issues.  Also the script generates a new disk map after the rebuild has begun so the map is
updated with the new drive information.  Note: Some slight enclosure path naming may need
customized on different system, change very minor.


~~ zfs_health_monitor ~~
Script to check on the health of all zpools in the system.  By default alerts are sent to the admin when
disks reach 90% full or if a zpool has encountered a read/write/checksum error.  Also drive temperatures are
monitored as well, with alerts sent when temperature of drive exceeds 45 degrees C. It is recommended to place
a soft link to this script in /etc/cron.hourly for the check.  Note that the frequency this script runs will
be the frequency with which alerts will be sent to the storage admin.


~~ zfs_scrub_runner ~~
A script to kick off parallel scrubs of all zpools on the system.  It is recommened to place a soft link to 
this script in /etc/cron.weekly for scrubbing, however if pool is too big/too slow/too critical the time period
may be adjusted.  Errors that are encountered by the scrub will be picked up by zfs_health_monitor and or
zfs_fault_finder depending on their nature and severity.


~~ zfs_vdev_creator ~~
Script that generates the /etc/zfs/vdev_id.conf file and generates the devices in /dev/disk/by-vdev/.  Script 
is custom to each enclosure, but this can serve as a framework for the enclosure that you are using.  This is 
to be run before your zpools are created, or can be run afterward, but all pools must be exported, and then 
re-imported using zfs import -d /dev/disk/by-vdev poolname.  


General Step By Step Use:
1. Modify zfs_enclosure_mapper to reflect the chassis in use
2. Modify zfs_vdev_creator with desired alias style and naming scheme, loops may need modified to fit the
   chassis style.  For example front and back naming
3. Run zfs_vdev_creator, then zfs_drive_mapper, put zfs_drive_mapper in /etc/rc.local
4. Create zpools using /dev/sdxx names, zfs export the pool, then zfs import -d /dev/disk/by-vdev poolname
5. Place zfs_fault_finder and zfs_health_monitor in /etc/cron.hourly
6. Place zfs_scrub_runner in /etc/cron.weekly
7. Run zfs_fault_fixer after replacing drive post failure, for best results wait 1-2 minutes post replacement
   to ensure that the drive has been picked up by Linux
